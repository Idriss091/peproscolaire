# Guide de Tests - PeproScolaire

Ce guide dÃ©crit la stratÃ©gie de tests mise en place pour le projet PeproScolaire, en particulier pour les modules d'Intelligence Artificielle.

## ğŸ“Š Vue d'ensemble

Le projet utilise une approche de tests multi-niveaux :
- **Tests unitaires** : Fonctions et composants individuels
- **Tests d'intÃ©gration** : Interactions entre modules
- **Tests End-to-End** : Workflows complets utilisateur
- **Tests de performance** : Benchmarks et optimisations

## ğŸ—ï¸ Structure des Tests

```
/tests
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ apps/ai_analytics/
â”‚   â”‚   â”œâ”€â”€ test_models.py          # Tests des modÃ¨les ML
â”‚   â”‚   â”œâ”€â”€ test_views.py           # Tests des APIs
â”‚   â”‚   â”œâ”€â”€ test_tasks.py           # Tests des tÃ¢ches Celery
â”‚   â”‚   â””â”€â”€ test_integration.py     # Tests d'intÃ©gration
â”‚   â””â”€â”€ requirements-test.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/stores/__tests__/       # Tests des stores Pinia
â”‚   â”œâ”€â”€ src/components/__tests__/   # Tests des composants Vue
â”‚   â”œâ”€â”€ src/api/__tests__/          # Tests des services API
â”‚   â””â”€â”€ vitest.config.ts
â””â”€â”€ scripts/
    â””â”€â”€ run-tests.sh               # Script de lancement global
```

## ğŸ”§ Configuration et Installation

### Backend (Django + Pytest)

```bash
cd backend
pip install -r requirements.txt
pip install pytest pytest-django coverage
```

### Frontend (Vue.js + Vitest)

```bash
cd frontend/peproscolaire-ui
npm install
npm install --save-dev @vue/test-utils vitest jsdom
```

## ğŸš€ Lancement des Tests

### Script Global (RecommandÃ©)

```bash
# Tous les tests
./scripts/run-tests.sh all

# Backend uniquement
./scripts/run-tests.sh backend

# Frontend uniquement
./scripts/run-tests.sh frontend

# Tests E2E
./scripts/run-tests.sh e2e
```

### Tests Backend SpÃ©cifiques

```bash
cd backend

# Tests des modÃ¨les IA
python manage.py test apps.ai_analytics.test_models

# Tests des APIs
python manage.py test apps.ai_analytics.test_views

# Tests des tÃ¢ches Celery
python manage.py test apps.ai_analytics.test_tasks

# Tests d'intÃ©gration
python manage.py test apps.ai_analytics.test_integration

# Avec couverture
coverage run --source='apps/ai_analytics' manage.py test apps.ai_analytics
coverage report --show-missing
coverage html
```

### Tests Frontend SpÃ©cifiques

```bash
cd frontend/peproscolaire-ui

# Tests des modules IA
npm run test:ai

# Tests des composants
npm run test:components

# Tests des APIs
npm run test:api

# Avec couverture
npm run test:coverage

# Mode watch
npm run test:watch
```

## ğŸ“‹ DÃ©tail des Tests ImplÃ©mentÃ©s

### ğŸ¤– Tests des Modules IA

#### Tests des ModÃ¨les ML (`test_models.py`)

- **DropoutRiskModelTest** : Tests du modÃ¨le de dÃ©tection de dÃ©crochage
  - Extraction des features (18 indicateurs)
  - PrÃ©dictions de risque (faible, modÃ©rÃ©, Ã©levÃ©)
  - Gestion des donnÃ©es manquantes
  - Importance des facteurs de risque

- **ModelTrainerTest** : Tests de l'entraÃ®nement
  - EntraÃ®nement du modÃ¨le avec donnÃ©es synthÃ©tiques
  - Ã‰valuation des performances (accuracy, precision, recall, F1)
  - GÃ©nÃ©ration de donnÃ©es d'entraÃ®nement
  - Gestion des erreurs d'entraÃ®nement

- **RiskProfileModelTest** : Tests du modÃ¨le de profil de risque
  - CrÃ©ation et mise Ã  jour des profils
  - Calcul automatique du niveau de risque
  - Relations avec les Ã©tudiants et annÃ©es scolaires

#### Tests des APIs (`test_views.py`)

- **GenerateAppreciationAPITest** : Tests de gÃ©nÃ©ration d'apprÃ©ciations
  - GÃ©nÃ©ration simple avec options personnalisÃ©es
  - Gestion des erreurs (Ã©tudiant inexistant, donnÃ©es invalides)
  - Validation des permissions

- **GenerateMultipleAppreciationsAPITest** : Tests de gÃ©nÃ©ration en lot
  - GÃ©nÃ©ration pour une classe entiÃ¨re
  - Gestion des Ã©checs partiels
  - Statistiques de gÃ©nÃ©ration

- **PredictStudentRiskAPITest** : Tests de prÃ©diction de risque
  - PrÃ©diction pour diffÃ©rents niveaux de risque
  - Facteurs de risque et recommandations
  - Cache des prÃ©dictions

- **AIModelStatusAPITest** : Tests du statut des modÃ¨les
  - RÃ©cupÃ©ration des performances des modÃ¨les
  - MÃ©triques globales du systÃ¨me
  - Ã‰tat des modÃ¨les (actif, erreur, entraÃ®nement)

#### Tests des TÃ¢ches Asynchrones (`test_tasks.py`)

- **MLModelTrainingTaskTest** : Tests d'entraÃ®nement des modÃ¨les
  - EntraÃ®nement rÃ©ussi et Ã©checs
  - Retry automatique en cas d'erreur temporaire
  - Types de modÃ¨les supportÃ©s

- **StudentRiskAnalysisTaskTest** : Tests d'analyse de risque
  - Analyse complÃ¨te d'un Ã©tudiant
  - DÃ©clenchement d'alertes automatiques
  - Gestion des donnÃ©es insuffisantes

- **BulkAnalysisTaskTest** : Tests d'analyse en masse
  - Analyse quotidienne automatique
  - DÃ©tection de patterns hebdomadaires
  - Performance sur de gros volumes

#### Tests d'IntÃ©gration (`test_integration.py`)

- **AIModulesIntegrationTest** : Workflows complets
  - Analyse â†’ Profil â†’ Alerte â†’ Intervention
  - GÃ©nÃ©ration d'apprÃ©ciations bout en bout
  - EntraÃ®nement et validation des modÃ¨les
  - SÃ©curitÃ© et permissions

- **PerformanceIntegrationTest** : Tests de performance
  - MÃ©triques du dashboard avec 50+ Ã©tudiants
  - Optimisation des requÃªtes SQL
  - Temps de rÃ©ponse des APIs critiques

### ğŸ–¥ï¸ Tests Frontend

#### Tests des Stores (`ai-modules.test.ts`)

- **State Management** : Tests de gestion d'Ã©tat
  - Initialisation avec valeurs par dÃ©faut
  - Gestion des Ã©tats de chargement et d'erreur
  - Mise Ã  jour rÃ©active des donnÃ©es

- **Model Status** : Tests du statut des modÃ¨les
  - RÃ©cupÃ©ration et mise Ã  jour du statut
  - PropriÃ©tÃ©s calculÃ©es (isDropoutModelActive)
  - Performance des modÃ¨les

- **Appreciation Generation** : Tests de gÃ©nÃ©ration
  - GÃ©nÃ©ration simple et multiple
  - Cache des apprÃ©ciations
  - Historique et gestion des limites

#### Tests des Composants (`AiDashboardView.test.ts`)

- **Component Rendering** : Tests de rendu
  - Affichage sans erreurs
  - MÃ©triques de performance des modÃ¨les
  - Distribution des risques
  - Ã‰tats de chargement

- **User Interactions** : Tests d'interactions
  - Boutons d'actions rapides
  - DÃ©clenchement d'analyses
  - Gestion des erreurs utilisateur

- **Reactive Updates** : Tests de rÃ©activitÃ©
  - Mise Ã  jour lors des changements de store
  - Recalcul des propriÃ©tÃ©s calculÃ©es

#### Tests des APIs (`ai-modules.test.ts`)

- **API Calls** : Tests des appels API
  - Tous les endpoints des modules IA
  - Gestion des erreurs HTTP
  - Authentification et permissions

- **Data Transformation** : Tests de transformation
  - Conversion des donnÃ©es API vers interface
  - Validation des types TypeScript
  - Gestion des rÃ©ponses malformÃ©es

## ğŸ“ˆ MÃ©triques et Couverture

### Objectifs de Couverture

- **Backend** : â‰¥ 90% de couverture de code
- **Frontend** : â‰¥ 85% de couverture de code
- **APIs critiques** : 100% de couverture

### MÃ©triques de Performance

- **Temps de rÃ©ponse API** : < 200ms pour les endpoints standards
- **Analyse de risque** : < 2s par Ã©tudiant
- **GÃ©nÃ©ration d'apprÃ©ciation** : < 5s par Ã©tudiant
- **Dashboard metrics** : < 1s avec 1000+ profils

## ğŸ”„ CI/CD et Automatisation

### GitHub Actions

Le projet utilise GitHub Actions pour l'automatisation :

```yaml
# .github/workflows/tests.yml
- Backend tests (PostgreSQL + Redis)
- Frontend tests (Node.js)
- Integration tests
- Security tests (Bandit, npm audit)
- Performance tests (Locust)
```

### Hooks PrÃ©-commit

```bash
# Installation des hooks
pre-commit install

# VÃ©rifications automatiques :
- Linting (flake8, ESLint)
- Formatting (black, prettier)
- Tests unitaires critiques
- SÃ©curitÃ© (bandit)
```

## ğŸ› Debugging et Troubleshooting

### ProblÃ¨mes Courants

#### Tests Backend

```bash
# Erreur de base de donnÃ©es
export DATABASE_URL="sqlite:///test.db"
python manage.py migrate

# Erreur de permissions
python manage.py collectstatic --noinput

# Erreur de dÃ©pendances
pip install -r requirements.txt
```

#### Tests Frontend

```bash
# Erreur de modules
npm install

# Erreur TypeScript
npm run type-check

# Cache Vitest
npx vitest run --reporter=verbose
```

### Debugging AvancÃ©

#### Tests ML en Mode Debug

```python
# Dans test_models.py
import pdb; pdb.set_trace()

# Ou avec logging
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### Tests Frontend en Mode Debug

```javascript
// Dans les tests Vitest
import { debug } from '@vue/test-utils'
debug() // Affiche le HTML du composant
```

## ğŸ“Š Rapports et Monitoring

### Rapports de Couverture

```bash
# Backend
open backend/htmlcov/index.html

# Frontend
open frontend/peproscolaire-ui/coverage/index.html
```

### Rapports de Performance

```bash
# Profiling Python
python -m cProfile manage.py test apps.ai_analytics

# Bundle analyzer Frontend
npm run build:analyze
```

## ğŸš€ Bonnes Pratiques

### Tests Backend

1. **Isolation** : Chaque test est indÃ©pendant
2. **Mocking** : Mock des services externes (OpenAI, etc.)
3. **Fixtures** : DonnÃ©es de test cohÃ©rentes
4. **Performance** : Tests de charge pour les analyses en masse

### Tests Frontend

1. **Component Testing** : Tests isolÃ©s des composants
2. **Store Testing** : Tests de la logique mÃ©tier
3. **API Mocking** : Mock des appels rÃ©seau
4. **Accessibility** : Tests d'accessibilitÃ©

### Tests d'IntÃ©gration

1. **ScÃ©narios RÃ©els** : Tests basÃ©s sur les cas d'usage
2. **DonnÃ©es CohÃ©rentes** : Jeux de donnÃ©es reprÃ©sentatifs
3. **Performance** : Benchmarks sur des volumes rÃ©els
4. **SÃ©curitÃ©** : Tests de permissions et validation

## ğŸ“š Ressources

- [Pytest Documentation](https://docs.pytest.org/)
- [Vitest Documentation](https://vitest.dev/)
- [Vue Test Utils](https://test-utils.vuejs.org/)
- [Django Testing](https://docs.djangoproject.com/en/4.2/topics/testing/)

---

> **Note** : Ce guide Ã©volue avec le projet. Consultez la documentation pour les mises Ã  jour.